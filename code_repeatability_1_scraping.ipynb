{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Repeatability Part 1: Scraping Twitter and Yahoo Finance Data\n",
    "### Note: This Notebook requires Python 3.8 or greater to run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary:\n",
    "\n",
    "This notebook is what we used to obtain all of our data and export it into .csv files.\n",
    "\n",
    "The environment used to run this notebook has a python version of 3.8.3. A version of 3.8 or greater is needed to run the development version of the python library snscrape. Snscrape is used to scrape twitter data. With the development version of Snscrape, we are able to include additional key features such as likecount, retweetcount, hashtags, cashtags, and much more.\n",
    "\n",
    "TThe first part of this notebook will pull twitter data using the snscrape library. We are pulling two datasets: 1 for specific users and 1 for specific hashtags. The users we are exploring are Elon Musk, Tesla, Gamestop, AMCTheatres, Bitcoin, Apompliano, Dogecoin, Mark Cuban, Shibetoshi Nakamoto (Creator of Dogecoin) and SpaceX. The tweets we are pulling based on hashtags are TSLA, Doge, AMC, BTC, and GME. However, we only ended up focusing on Bitcoin in the end. We have decided to pull all tweets from 2019 - Current.\n",
    "\n",
    "The second part of this notebook will scrape historical stock information directly from yahoo finance. It scrapes historical stock open & close prices, volume, and other attributes from the Bitcoin ticker. The data is scraped into two different dataframes. One that pulls historical information dating back 5 years on a daily basis. Another that pulls historical information dating back  months on an hourly basis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull Twitter Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If necessary, install the libraries that we used for scraping Twitter and Yahoo Finance Data,\n",
    "# snscrape and yfinance\n",
    "# For snscrape, the development version is necessary in order to get all the features we need\n",
    "\n",
    "#pip3 install git+https://github.com/JustAnotherArchivist/snscrape.git\n",
    "#pip install yfinance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "import os\n",
    "import time\n",
    "import yfinance as yf\n",
    "startTime = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of twitter users and hashtags to pull tweets from\n",
    "twitters = [\n",
    "    'elonmusk',\n",
    "    'Tesla',\n",
    "    'Gamestop',\n",
    "    'AMCTheatres',\n",
    "    'Bitcoin',\n",
    "    'APompliano',\n",
    "    'Dogecoin',\n",
    "    'Mcuban',\n",
    "    'BillyM2k',\n",
    "    'SpaceX',\n",
    "    ]\n",
    "\n",
    "hashtags =[\n",
    "    '#TSLA',\n",
    "    '#DOGE',\n",
    "    '#AMC',\n",
    "    '#BTC',\n",
    "    '#GME'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elonmusk\n",
      "Tesla\n",
      "Gamestop\n",
      "AMCTheatres\n",
      "Bitcoin\n",
      "APompliano\n",
      "Dogecoin\n",
      "Mcuban\n",
      "BillyM2k\n",
      "SpaceX\n"
     ]
    }
   ],
   "source": [
    "# scrape tweets from users. Will iterate over all values and save as a json file. \n",
    "# We will use the pandas library to read each json file in as a dataframe.\n",
    "# Pulling from the start of January 1st, 2019 for each user\n",
    "for user in twitters:\n",
    "    os.system(\"snscrape --jsonl --since 2019-01-01 twitter-search 'from:\" + user + \"'> \" + user + \".json\")\n",
    "    print(user) # printing each user to see when all tweets are scraped for a user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing a dataframe to store tweets scraped with above code for users\n",
    "user_tweets = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterating over every user and appending scraped tweets to the user_tweets dataframe\n",
    "for user in twitters:\n",
    "    df = pd.read_json(user+'.json',lines = True)\n",
    "    df['keyword_search'] = user #creating a column to show what value was used to search tweets\n",
    "    user_tweets = user_tweets.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#TSLA\n",
      "#DOGE\n",
      "#AMC\n",
      "#BTC\n",
      "#GME\n"
     ]
    }
   ],
   "source": [
    "# scraping tweets for each hashtag in hashtags list\n",
    "# This is slightly different from how we did it\n",
    "# We did not use the --until 2021-06-15 statement, but that is the last day of our data\n",
    "# because that is the day we scraped it\n",
    "for tag in hashtags:\n",
    "    os.system(\"snscrape --jsonl --since 2019-01-01 --until 2021-06-15 twitter-search 'from:\" + tag + \"'> \" + tag + \".json\")\n",
    "    print(tag) # printing each hashtag to see when all tweets are scraped for a hashtag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing a dataframe to store tweets scraped with above code for hashtags\n",
    "hashtag_tweets = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterating over every hashtag and appending scraped tweets to the hashtag_tweets dataframe\n",
    "for tag in hashtags:\n",
    "    df2 = pd.read_json(tag +'.json',lines = True)\n",
    "    df2['keyword_search'] = tag #creating a column to show what value was used to search tweets\n",
    "    hashtag_tweets = hashtag_tweets.append(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keeping only certain columns of interest\n",
    "user_tweets = user_tweets[['date','content','id','user','replyCount','retweetCount','likeCount',\n",
    "             'quoteCount','lang','inReplyToTweetId','inReplyToUser','mentionedUsers',\n",
    "             'coordinates','place','hashtags','cashtags','keyword_search']]\n",
    "hashtag_tweets = hashtag_tweets[['date','content','id','user','replyCount','retweetCount','likeCount',\n",
    "             'quoteCount','lang','inReplyToTweetId','inReplyToUser','mentionedUsers',\n",
    "             'coordinates','place','hashtags','cashtags','keyword_search']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporting dataframes to csv\n",
    "user_tweets.to_csv(\"user_tweets.csv\")\n",
    "hashtag_tweets.to_csv(\"hashtag_tweets.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time in seconds: 5345.701666355133\n"
     ]
    }
   ],
   "source": [
    "# determining how long script takes to run in seconds\n",
    "executionTime = (time.time() - startTime)\n",
    "print('Execution time in seconds: ' + str(executionTime))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull Yahoo Finance Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bitcoin\n",
    "stock_strings = ['BTC-USD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "# creating a dataframe for last 5 years of stock for above stock\n",
    "df_list = []\n",
    "for ticker in stock_strings:\n",
    "    data = yf.download(ticker, group_by=\"Ticker\", period='5y')\n",
    "    data['ticker'] = ticker  # add this column because the dataframe doesn't contain a column with the ticker\n",
    "    df_list.append(data)\n",
    "\n",
    "# combine all dataframes into a single dataframe\n",
    "df = pd.concat(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "# creating a dataframe for last 2 months every hour\n",
    "df_hour_list = []\n",
    "for ticker in stock_strings:\n",
    "    data2 = yf.download(ticker, group_by=\"Ticker\", period='8mo',interval='1h')\n",
    "    data2['ticker'] = ticker  # add this column becasue the dataframe doesn't contain a column with the ticker\n",
    "    df_hour_list.append(data2)\n",
    "\n",
    "# combine all dataframes into a single dataframe\n",
    "df_hour = pd.concat(df_hour_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change directory\n",
    "os.chdir('../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save files\n",
    "df.to_csv('casestudy_data/group_9/daily_stock_last5yr.csv')\n",
    "df_hour.to_csv('casestudy_data/group_9/hourly_stock_last8mo.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
